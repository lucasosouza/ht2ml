{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm with input dim 3 and output dim 3\n",
    "lstm = nn.LSTM(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.5525,  0.6355, -0.3968]]),\n",
       " tensor([[-0.6571, -1.6428,  0.9803]]),\n",
       " tensor([[-0.0421, -0.8206,  0.3133]]),\n",
       " tensor([[-1.1352,  0.3773, -0.2824]]),\n",
       " tensor([[-2.5667, -1.4303,  0.5009]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define inputs\n",
    "inputs = [torch.randn(1,3) for _ in range(5)]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.5438, -0.4057,  1.1341]]]),\n",
       " tensor([[[-1.1115,  0.3501, -0.7703]]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state\n",
    "# 3 dimensions: \n",
    "# 1 the sequence itself, 2 the indexes in batch, 3 the indexes in the input\n",
    "hidden = (torch.randn(1,1,3), torch.randn(1,1,3 ))\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each input\n",
    "for i in inputs:\n",
    "    # get output and hidden layer\n",
    "    # hidden layer is passed to the next input, but in this example only the output matters\n",
    "    out, hidden = lstm(i.view(1,1,-1), hidden)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative\n",
    "inputs = torch.randn(5,1,3)\n",
    "# torch.cat(inputs) - alternative way to transform the original input - use concat\n",
    "# hidden is a two tuple with two set of weights - presumably one for the current state, and one for last state\n",
    "# what about weights from lstm gates?\n",
    "\n",
    "hidden = (torch.randn(1,1,3), torch.randn(1,1,3 ))\n",
    "out, hidden = lstm(inputs.view(5,1,3), hidden)\n",
    "# output is the same format as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1473,  0.6272,  1.0935]],\n",
       "\n",
       "        [[ 0.0939,  1.2381, -1.3459]],\n",
       "\n",
       "        [[ 0.5119, -0.6933, -0.1668]],\n",
       "\n",
       "        [[-0.9999, -1.6476,  0.8098]],\n",
       "\n",
       "        [[ 0.0554,  1.1340, -0.5326]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6383, -0.1405, -0.1033]],\n",
       "\n",
       "        [[-0.1460, -0.0367, -0.2437]],\n",
       "\n",
       "        [[-0.4672, -0.0584, -0.2693]],\n",
       "\n",
       "        [[-0.5145,  0.0216, -0.2656]],\n",
       "\n",
       "        [[-0.1504,  0.0266, -0.1379]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1504,  0.0266, -0.1379]]], grad_fn=<StackBackward>),\n",
       " tensor([[[-0.9364,  0.0484, -0.4176]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM for part-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the log softmax of the affine map of the hidden state\n",
    "# the predicted tag is the tag that the maximum value in thisvector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "{'DET': 0, 'NN': 1, 'V': 2}\n"
     ]
    }
   ],
   "source": [
    "# convert text to numbers\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# training is sequence of words and their tags\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix) # always next in sequence\n",
    "            \n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "print(tag_to_ix)\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        # initialize the hidden dimension\n",
    "        # what is this for? seem to have no use at all\n",
    "        # self.hidden_dim = hidden_dim\n",
    "\n",
    "        # pass vocabulary through and embedding layer\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        # missing a linear layer\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # convert sentence into embeddings\n",
    "        # this part would be optional, can I do directly with one hot encoding?\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        # get output from lstm - what about the hidden layer?\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # convert lstm output to tag space, linear transformation\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        # apply softmax to get the probabilities\n",
    "        # missing a softmax if doing classification\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([0, 1, 2, 0, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tags = prepare_sequence(training_data[0][1], tag_to_ix)\n",
    "inputs, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2])\n",
      "tensor([0, 1, 2, 0, 1])\n",
      "tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(torch.argmax(tag_scores, dim=1))\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    targets = prepare_sequence(training_data[0][1], tag_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(torch.argmax(tag_scores, dim=1))\n",
    "    print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example - can lstm learn the sine function from 10 examples of sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "class LSTMSeq(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_length, hidden_dim, target_size):\n",
    "        super(LSTMSeq, self).__init__()\n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(seq_length, hidden_dim)\n",
    "        self.non_linearity = nn.ReLU()        \n",
    "        self.fc1 = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        self.fc2 = nn.Linear(int(hidden_dim/2), int(hidden_dim/5))\n",
    "        self.output_layer = nn.Linear(int(hidden_dim/5), target_size)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        n_samples = 1\n",
    "        if type(sequence) == list:\n",
    "            n_samples = len(sequence)\n",
    "            sequence = torch.cat(sequence)        \n",
    "        lstm_out, _ = self.lstm_layer(sequence.view(n_samples,1,-1)) # lstm\n",
    "        x = self.fc1(lstm_out) # hidden\n",
    "        x = self.non_linearity(x) # activation\n",
    "        x = self.fc2(x) # hidden\n",
    "        x = self.non_linearity(x) # activation\n",
    "        predictions = self.output_layer(x) # output\n",
    "        return predictions\n",
    "\n",
    "# lstm has 3 dimensions: \n",
    "# 1 the sequence itself, 2 the indexes in batch, 3 the indexes in the input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0441, -0.0280, -0.0333,  0.0033, -0.0195,  0.0507,  0.0914, -0.0480,\n",
       "        -0.0082,  0.0422], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparams\n",
    "n_samples = 20\n",
    "hidden_dim = 500\n",
    "seq_length = 10\n",
    "batch_size = 1\n",
    "\n",
    "# define input\n",
    "inputs = [ torch.FloatTensor(range(i*seq_length, (i+1)*seq_length)) for i in range(n_samples) ]\n",
    "targets = [torch.sin(i) for i in inputs]\n",
    "# inputs, targets, inputs[0].dtype, targets[0].dtype\n",
    "\n",
    "# define model\n",
    "model = LSTMSeq(seq_length=seq_length, hidden_dim=hidden_dim, target_size=seq_length)\n",
    "model(inputs[0]).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "# targets\n",
    "# r2_score(model(inputs).squeeze().tolist(), [t.tolist() for t in targets])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize\n",
    "epochs = 1000\n",
    "loss_func = nn.MSELoss()\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # training loop\n",
    "    cum_loss = 0\n",
    "    for input, target in zip(inputs, targets):\n",
    "        # setup\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # calculate loss\n",
    "        prediction = model(input).squeeze()\n",
    "        loss = loss_func(prediction, target)\n",
    "        cum_loss += loss.item()\n",
    "        \n",
    "        # gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "    # report\n",
    "    if epoch % (epochs/20) == 0:   \n",
    "        # report\n",
    "        with torch.no_grad():            \n",
    "            # get r2score\n",
    "            score = r2_score(model(inputs).squeeze().tolist(), [t.tolist() for t in targets])\n",
    "            # print loss\n",
    "            print(\"Epoch: {:d}, Loss: {:.4f}, R2: {:.4f}\".format(epoch, cum_loss, score))\n",
    "\n",
    "    # decay 5 times during training\n",
    "    if epoch % (epochs/4) == 0:\n",
    "        # decay lr\n",
    "        lr = lr/2\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5043, R2: -11505.4866\n",
      "Epoch: 400, Loss: 0.4915, R2: -591.8935\n",
      "Epoch: 800, Loss: 0.4732, R2: -103.6741\n",
      "Epoch: 1200, Loss: 0.4353, R2: -14.1146\n",
      "Epoch: 1600, Loss: 0.4070, R2: -5.4346\n",
      "Epoch: 2000, Loss: 0.3943, R2: -3.2128\n",
      "Epoch: 2400, Loss: 0.3751, R2: -2.8642\n",
      "Epoch: 2800, Loss: 0.3729, R2: -2.1854\n",
      "Epoch: 3200, Loss: 0.3616, R2: -2.2626\n",
      "Epoch: 3600, Loss: 0.3471, R2: -1.3999\n",
      "Epoch: 4000, Loss: 0.3388, R2: -1.2749\n",
      "Epoch: 4400, Loss: 0.3311, R2: -1.0940\n",
      "Epoch: 4800, Loss: 0.3267, R2: -1.0602\n",
      "Epoch: 5200, Loss: 0.3226, R2: -1.0883\n",
      "Epoch: 5600, Loss: 0.3215, R2: -0.9724\n",
      "Epoch: 6000, Loss: 0.3195, R2: -0.9515\n",
      "Epoch: 6400, Loss: 0.3196, R2: -0.8198\n",
      "Epoch: 6800, Loss: 0.3142, R2: -0.7672\n",
      "Epoch: 7200, Loss: 0.3087, R2: -0.7866\n",
      "Epoch: 7600, Loss: 0.3035, R2: -0.6671\n",
      "Epoch: 8000, Loss: 0.2955, R2: -0.6621\n",
      "Epoch: 8400, Loss: 0.2839, R2: -0.4455\n",
      "Epoch: 8800, Loss: 0.2801, R2: -0.4094\n",
      "Epoch: 9200, Loss: 0.2777, R2: -0.4614\n",
      "Epoch: 9600, Loss: 0.2733, R2: -0.3094\n",
      "Epoch: 10000, Loss: 0.2841, R2: -0.3669\n",
      "Epoch: 10400, Loss: 0.2727, R2: -0.3345\n",
      "Epoch: 10800, Loss: 0.2590, R2: -0.1692\n",
      "Epoch: 11200, Loss: 0.2625, R2: -0.2542\n",
      "Epoch: 11600, Loss: 0.2575, R2: -0.1226\n",
      "Epoch: 12000, Loss: 0.2550, R2: -0.1333\n",
      "Epoch: 12400, Loss: 0.2489, R2: -0.0565\n",
      "Epoch: 12800, Loss: 0.2476, R2: -0.0450\n",
      "Epoch: 13200, Loss: 0.2463, R2: -0.0152\n",
      "Epoch: 13600, Loss: 0.2456, R2: -0.0098\n",
      "Epoch: 14000, Loss: 0.2436, R2: 0.0069\n",
      "Epoch: 14400, Loss: 0.2415, R2: 0.0087\n",
      "Epoch: 14800, Loss: 0.2410, R2: 0.0176\n",
      "Epoch: 15200, Loss: 0.2388, R2: 0.0240\n",
      "Epoch: 15600, Loss: 0.2411, R2: 0.0172\n",
      "Epoch: 16000, Loss: 0.2352, R2: 0.0562\n",
      "Epoch: 16400, Loss: 0.2333, R2: 0.0703\n",
      "Epoch: 16800, Loss: 0.2311, R2: 0.0893\n",
      "Epoch: 17200, Loss: 0.2285, R2: 0.0994\n",
      "Epoch: 17600, Loss: 0.2265, R2: 0.1133\n",
      "Epoch: 18000, Loss: 0.2246, R2: 0.1336\n",
      "Epoch: 18400, Loss: 0.2227, R2: 0.1444\n",
      "Epoch: 18800, Loss: 0.2212, R2: 0.1544\n",
      "Epoch: 19200, Loss: 0.2193, R2: 0.1733\n",
      "Epoch: 19600, Loss: 0.2179, R2: 0.1783\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "epochs = 20000\n",
    "loss_func = nn.MSELoss()\n",
    "lr = 0.02\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # training loop\n",
    "    cum_loss = 0\n",
    "    # setup\n",
    "    model.zero_grad()\n",
    "\n",
    "    # calculate loss\n",
    "    prediction = model(inputs).squeeze()\n",
    "    loss = loss_func(prediction, torch.stack(targets))\n",
    "    cum_loss += loss.item()\n",
    "\n",
    "    # gradient descent\n",
    "    loss.backward()\n",
    "    optimizer.step()        \n",
    "\n",
    "    # report\n",
    "    if epoch % (epochs/50) == 0:   \n",
    "        # report\n",
    "        with torch.no_grad():            \n",
    "            # get r2score\n",
    "            score = r2_score(model(inputs).squeeze().tolist(), [t.tolist() for t in targets])\n",
    "            # print loss\n",
    "            print(\"Epoch: {:d}, Loss: {:.4f}, R2: {:.4f}\".format(epoch, cum_loss, score))\n",
    "\n",
    "    # decay 5 times during training\n",
    "    if epoch % (epochs/5) == 0:\n",
    "        # decay lr\n",
    "        lr = lr/2\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1954750884735427"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(model(inputs).squeeze().tolist(), [t.tolist() for t in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999892388821722\n",
      "-2.8105100120405004\n",
      "-3.3776683280406443\n",
      "-96.57121065826844\n",
      "-27.962900137429855\n",
      "-26.609077214351046\n",
      "-92.43727449613884\n",
      "-71.27234776600149\n",
      "-36.27828033688154\n",
      "-58.56023416832904\n",
      "-38.62535917725062\n",
      "-62.088026943689265\n",
      "-114.56060773145401\n",
      "-228.71235501962315\n",
      "-996.452558536015\n",
      "-689.045211578841\n",
      "-141.55061614845198\n",
      "-70.77711555111841\n",
      "-42.28026052794875\n",
      "-16.900732602288244\n"
     ]
    }
   ],
   "source": [
    "# for some reason the model is overfitting to the first examples\n",
    "# what if I randomly sort at each start, or just learn all at once\n",
    "for input, target in zip(inputs, targets):\n",
    "    print(r2_score(model(input).squeeze().tolist(), target.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.004310354590415955, 0.0),\n",
       " (0.8413498401641846, 0.8414709568023682),\n",
       " (0.9114561080932617, 0.9092974066734314),\n",
       " (0.1414901316165924, 0.14112000167369843),\n",
       " (-0.7538073062896729, -0.756802499294281),\n",
       " (-0.9602295756340027, -0.9589242935180664),\n",
       " (-0.27775439620018005, -0.279415488243103),\n",
       " (0.6562947630882263, 0.6569865942001343),\n",
       " (0.9893290996551514, 0.9893582463264465),\n",
       " (0.41515031456947327, 0.41211849451065063)]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model(inputs[0]).squeeze().tolist(), targets[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.16154761612415314, 0.997799277305603),\n",
       " (-0.05912990868091583, 0.5949085354804993),\n",
       " (0.09348908066749573, -0.3549383580684662),\n",
       " (0.13962914049625397, -0.9784565567970276),\n",
       " (0.07361795008182526, -0.7023863196372986),\n",
       " (-0.054895758628845215, 0.21945466101169586),\n",
       " (-0.14959675073623657, 0.939530074596405),\n",
       " (-0.08419793844223022, 0.7958058714866638),\n",
       " (0.04853467643260956, -0.07957859337329865),\n",
       " (0.13360965251922607, -0.8817988634109497)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model(inputs[-11]).squeeze().tolist(), targets[-1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([373., 376., 379., 382., 385., 388., 391., 394., 397., 400.]),\n",
       " tensor([ 0.7510, -0.8366,  0.9056, -0.9564,  0.9880, -0.9999,  0.9918, -0.9638,\n",
       "          0.9165, -0.8509]))"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.FloatTensor(np.arange(373, 403, 3))\n",
    "target = torch.sin(input)\n",
    "input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([373., 376., 379., 382., 385., 388., 391., 394., 397., 400.]),\n",
       " tensor([ 0.7510, -0.8366,  0.9056, -0.9564,  0.9880, -0.9999,  0.9918, -0.9638,\n",
       "          0.9165, -0.8509]))"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.367722749710083, 0.7509673237800598),\n",
       " (0.022573277354240417, -0.8366391062736511),\n",
       " (-0.3374350965023041, 0.9055655598640442),\n",
       " (-0.4086197316646576, -0.9563671350479126),\n",
       " (-0.0877951830625534, 0.9880269765853882),\n",
       " (0.3275204300880432, -0.9999114871025085),\n",
       " (0.4304143786430359, 0.9917827248573303),\n",
       " (0.15984366834163666, -0.9638034105300903),\n",
       " (-0.2661747932434082, 0.9165335893630981),\n",
       " (-0.44398248195648193, -0.8509193658828735)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(model(input).squeeze().tolist(), target.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.750225448120924"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(model(input).squeeze().tolist(), target.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
